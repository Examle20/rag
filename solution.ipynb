{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a5ff1d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Optional\n",
    "import os\n",
    "import faiss\n",
    "import numpy as np\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "import ijson\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa911cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"arxiv-metadata-s.json\"\n",
    "MODEL = \"Qwen/Qwen3-Embedding-0.6B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL, use_fast=True)\n",
    "\n",
    "lens = []\n",
    "with open(PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    for obj in ijson.items(f, \"item\"):\n",
    "        title = (obj.get(\"title\") or \"\").strip()\n",
    "        abstract = (obj.get(\"abstract\") or \"\").strip()\n",
    "        text = (title + \"\\n\" + abstract).strip()\n",
    "        ids = tokenizer(text, add_special_tokens=True, truncation=False)[\"input_ids\"]\n",
    "        lens.append(len(ids))\n",
    "\n",
    "arr = np.array(lens, dtype=np.int32)\n",
    "print(\"count:\", arr.size)\n",
    "print(\"mean tokens:\", float(arr.mean()))\n",
    "for p in [50, 90, 95, 99, 99.5, 99.9]:\n",
    "    print(f\"p{p}:\", float(np.percentile(arr, p)))\n",
    "print(\"max:\", int(arr.max()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2663b6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "class RAG:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedder_name: str = \"Qwen/Qwen3-Embedding-0.6B\",\n",
    "        reranker_name: str = \"Qwen/Qwen3-Reranker-0.6B\",\n",
    "        chunk_size: int = 500,\n",
    "        chunk_overlap: int = 125,\n",
    "        device: Optional[str] = None,\n",
    "    ):\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.emb_tokenizer = AutoTokenizer.from_pretrained(embedder_name)\n",
    "        self.embedder = AutoModel.from_pretrained(embedder_name).to(self.device)\n",
    "        self.embedder.eval()\n",
    "\n",
    "        self.rr_tokenizer = AutoTokenizer.from_pretrained(reranker_name, padding_side='left')\n",
    "        self.reranker = AutoModelForCausalLM.from_pretrained(reranker_name).to(self.device)\n",
    "        self.reranker.eval()\n",
    "\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap,)\n",
    "        self.index = None\n",
    "        self.doc_store = []\n",
    "\n",
    "        self.max_length = 1024\n",
    "        self.token_false_id = self.rr_tokenizer.convert_tokens_to_ids(\"no\")\n",
    "        self.token_true_id = self.rr_tokenizer.convert_tokens_to_ids(\"yes\")\n",
    "        prefix = \"<|im_start|>system\\nJudge whether the Document meets the requirements based on the Query and the Instruct provided. Note that the answer can only be \\\"yes\\\" or \\\"no\\\".<|im_end|>\\n<|im_start|>user\\n\"\n",
    "        suffix = \"<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n\"\n",
    "        self.prefix_tokens = self.rr_tokenizer.encode(prefix, add_special_tokens=False)\n",
    "        self.suffix_tokens = self.rr_tokenizer.encode(suffix, add_special_tokens=False)\n",
    "\n",
    "    def _generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        inputs = self.emb_tokenizer(\n",
    "            texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=self.max_length,\n",
    "        ).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.embedder(**inputs)\n",
    "\n",
    "        embeddings = self.last_token_pool(outputs.last_hidden_state, inputs.attention_mask)\n",
    "        embeddings = embeddings.float().cpu()  # <-- ключевая строка\n",
    "\n",
    "        return F.normalize(embeddings, p=2, dim=1).numpy()\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def last_token_pool(last_hidden_states: Tensor,\n",
    "                        attention_mask: Tensor) -> Tensor:\n",
    "        left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n",
    "        if left_padding:\n",
    "            return last_hidden_states[:, -1]\n",
    "        else:\n",
    "            sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "            batch_size = last_hidden_states.shape[0]\n",
    "            return last_hidden_states[\n",
    "                torch.arange(batch_size, device=last_hidden_states.device),\n",
    "                sequence_lengths] \n",
    "\n",
    "    def load_and_process_arxiv_json(self, file_path: str, split: bool = False) -> List[Document]:\n",
    "        ext = os.path.splitext(file_path)[1].lower()\n",
    "        if ext != \".json\":\n",
    "            raise ValueError(f\"Expected .json file, got: {ext}\")\n",
    "\n",
    "        docs: List[Document] = []\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for obj in ijson.items(f, \"item\"):\n",
    "                arxiv_id = obj.get(\"id\")\n",
    "                title = (obj.get(\"title\") or \"\").strip()\n",
    "                abstract = (obj.get(\"abstract\") or \"\").strip()\n",
    "                text = (title + \"\\n\" + abstract).strip()\n",
    "                meta = {\n",
    "                    \"id\": arxiv_id,\n",
    "                    \"title\": title,\n",
    "                    \"categories\": obj.get(\"categories\"),\n",
    "                    \"doi\": obj.get(\"doi\"),\n",
    "                    \"journal_ref\": obj.get(\"journal-ref\"),\n",
    "                    \"update_date\": obj.get(\"update_date\"),\n",
    "                }\n",
    "\n",
    "                docs.append(Document(page_content=text, metadata=meta))\n",
    "\n",
    "        return self.text_splitter.split_documents(docs) if split else docs\n",
    "\n",
    "    def build_index(self, file_path: str, batch_size: int = 64) -> None:\n",
    "        all_docs = self.load_and_process_arxiv_json(file_path, split=False)\n",
    "        self.doc_store = all_docs\n",
    "        embs = []\n",
    "        for i in tqdm(range(0, len(all_docs), batch_size), desc=\"Embedding corpus\", unit=\"batch\"):\n",
    "            batch_texts = [d.page_content for d in all_docs[i:i + batch_size]]\n",
    "            embs.append(self._generate_embeddings(batch_texts))\n",
    "        embeddings = np.concatenate(embs, axis=0).astype(\"float32\")\n",
    "        self.index = faiss.IndexFlatIP(embeddings.shape[1])\n",
    "        self.index.add(embeddings)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_detailed_instruct(task_description: str, query: str):\n",
    "        return f'Instruct: {task_description}\\nQuery:{query}'\n",
    "\n",
    "    @staticmethod\n",
    "    def format_reranker_instruction(query, doc, instruction=None):\n",
    "        if instruction is None:\n",
    "            instruction = 'Given a web search query, retrieve relevant passages that answer the query'\n",
    "        output = \"<Instruct>: {instruction}\\n<Query>: {query}\\n<Document>: {doc}\".format(\n",
    "            instruction=instruction, query=query, doc=doc)\n",
    "        return output\n",
    "\n",
    "    def process_inputs(self, pairs):\n",
    "        \"\"\"Обработка данных для реранкера\"\"\"\n",
    "        inputs = self.rr_tokenizer(pairs,\n",
    "                                   padding=False,\n",
    "                                   truncation='longest_first',\n",
    "                                   return_attention_mask=False,\n",
    "                                   max_length=self.max_length -\n",
    "                                   len(self.prefix_tokens) -\n",
    "                                   len(self.suffix_tokens))\n",
    "        for i, ele in enumerate(inputs['input_ids']):\n",
    "            inputs['input_ids'][\n",
    "                i] = self.prefix_tokens + ele + self.suffix_tokens\n",
    "        inputs = self.rr_tokenizer.pad(inputs,\n",
    "                                       padding=True,\n",
    "                                       return_tensors=\"pt\",\n",
    "                                       max_length=self.max_length)\n",
    "\n",
    "        # переносим тензоры на девайс ранжирующей модели\n",
    "        for key in inputs:\n",
    "            inputs[key] = inputs[key].to(self.device)\n",
    "        return inputs\n",
    "\n",
    "    def search(self,\n",
    "               query: str,\n",
    "               k: int = 5,\n",
    "               task: str = None):\n",
    "        if self.index is None:\n",
    "            raise ValueError(\"Index not initialized\")\n",
    "\n",
    "        if task is None:\n",
    "            task = 'Given a web search query, retrieve relevant passages that answer the query'\n",
    "\n",
    "        query_embedding = self._generate_embeddings([query])\n",
    "        distances, indices = self.index.search(query_embedding, k)\n",
    "        return distances, indices         \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def compute_logits(self, inputs):\n",
    "        batch_scores = self.reranker(**inputs).logits[:, -1, :]\n",
    "        true_vector = batch_scores[:, self.token_true_id]\n",
    "        false_vector = batch_scores[:, self.token_false_id]\n",
    "        batch_scores = torch.stack([false_vector, true_vector], dim=1)\n",
    "        batch_scores = torch.nn.functional.log_softmax(batch_scores, dim=1)\n",
    "        scores = batch_scores[:, 1].exp().tolist()\n",
    "        return scores\n",
    "\n",
    "    def rerank(self, query: str, documents: List[str], batch_size=4):\n",
    "        pairs = []\n",
    "        for d in documents:\n",
    "            pairs.append(self.format_reranker_instruction(query, d))\n",
    "\n",
    "        scores = []\n",
    "        for i in range(0, len(pairs), batch_size):\n",
    "            inputs = self.process_inputs(pairs[i:i + batch_size])\n",
    "            sc = self.compute_logits(inputs)\n",
    "            scores.extend(sc)\n",
    "        return scores\n",
    "    \n",
    "def save(self, prefix: str) -> None:\n",
    "    if self.index is None:\n",
    "        raise ValueError(\"Index not initialized\")\n",
    "    faiss.write_index(self.index, prefix + \".index\")\n",
    "    with open(prefix + \".docstore.pkl\", \"wb\") as f:\n",
    "        pickle.dump(self.doc_store, f)\n",
    "\n",
    "def load(self, prefix: str) -> None:\n",
    "    self.index = faiss.read_index(prefix + \".index\")\n",
    "    with open(prefix + \".docstore.pkl\", \"rb\") as f:\n",
    "        self.doc_store = pickle.load(f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6619bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"A trap under radiation of two traveling\"\n",
    "\n",
    "k = 5\n",
    "rag = RAG(device=\"cuda\")\n",
    "rag.build_index(\"./arxiv-metadata-s.json\")\n",
    "\n",
    "D, I = rag.search(q, k=k)\n",
    "candidates = [rag.doc_store[i].page_content for i in I[0]]\n",
    "\n",
    "for c in candidates:\n",
    "    print(c[:800])\n",
    "    print(\"-#\" * 20)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f8057e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded. docs: 98213 index size: 98213\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import pickle\n",
    "\n",
    "prefix = \"arxiv_rag_qwen3\"\n",
    "\n",
    "rag = RAG(device=\"cuda\")  # или cpu\n",
    "rag.index = faiss.read_index(prefix + \".index\")\n",
    "\n",
    "with open(prefix + \".docstore.pkl\", \"rb\") as f:\n",
    "    rag.doc_store = pickle.load(f)\n",
    "\n",
    "print(\"Loaded. docs:\", len(rag.doc_store), \"index size:\", rag.index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a4804781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faiss dim: 1024\n",
      "index vectors: 98213\n",
      "approx faiss vectors size (MB): 383.64453125\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "ntotal = rag.index.ntotal\n",
    "dim = rag.index.d\n",
    "bytes_index = ntotal * dim * 4  # float32 = 4 bytes\n",
    "\n",
    "print(\"faiss dim:\", dim)\n",
    "print(\"index vectors:\", ntotal)\n",
    "print(\"approx faiss vectors size (MB):\", bytes_index / (1024**2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "631b3699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: arxiv_rag_qwen3.index and arxiv_rag_qwen3.docstore.pkl\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import pickle\n",
    "\n",
    "prefix = \"arxiv_rag_qwen3\"\n",
    "\n",
    "# 1) сохранить FAISS индекс\n",
    "faiss.write_index(rag.index, prefix + \".index\")\n",
    "\n",
    "# 2) сохранить doc_store (список Document)\n",
    "with open(prefix + \".docstore.pkl\", \"wb\") as f:\n",
    "    pickle.dump(rag.doc_store, f)\n",
    "\n",
    "print(\"Saved:\", prefix + \".index\", \"and\", prefix + \".docstore.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "371645ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def profile_query(rag, query: str, retrieve_k: int = 50, rr_batch_size: int = 4):\n",
    "    if rag.index is None:\n",
    "        raise ValueError(\"Index not initialized\")\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    q_emb = rag._generate_embeddings([query])\n",
    "    t1 = time.perf_counter()\n",
    "\n",
    "    D, I = rag.index.search(q_emb.astype(\"float32\"), retrieve_k)\n",
    "    t2 = time.perf_counter()\n",
    "\n",
    "    idxs = [int(x) for x in I[0]]\n",
    "    cand_docs = [rag.doc_store[i] for i in idxs]\n",
    "    cand_texts = [d.page_content for d in cand_docs]\n",
    "    t3 = time.perf_counter()\n",
    "\n",
    "    scores = rag.rerank(query, cand_texts, batch_size=rr_batch_size)\n",
    "    t4 = time.perf_counter()\n",
    "\n",
    "    ranked = sorted(zip(idxs, scores), key=lambda x: x[1], reverse=True)[:5]\n",
    "    t5 = time.perf_counter()\n",
    "\n",
    "    return {\n",
    "        \"query_embed_s\": t1 - t0,\n",
    "        \"faiss_search_s\": t2 - t1,\n",
    "        \"gather_candidates_s\": t3 - t2,\n",
    "        \"rerank_s\": t4 - t3,\n",
    "        \"sort_top5_s\": t5 - t4,\n",
    "        \"total_s\": t5 - t0,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa1d2ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query_embed_s': 0.03334781900048256, 'faiss_search_s': 0.0300979189996724, 'gather_candidates_s': 9.762999980011955e-05, 'rerank_s': 1.567501509000067, 'sort_top5_s': 2.010000025620684e-05, 'total_s': 1.6310649770002783}\n"
     ]
    }
   ],
   "source": [
    "profile = profile_query(rag, \"attention mechanism in transformers\", retrieve_k=50, rr_batch_size=4)\n",
    "print(profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fcfe670",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def mrr_at_5(rag, test_csv_path: str, retrieve_k: int = 50, rr_batch_size: int = 4, limit: int | None = None):\n",
    "    df = pd.read_csv(test_csv_path)\n",
    "    if limit is not None:\n",
    "        df = df.head(limit)\n",
    "\n",
    "    mrrs = []\n",
    "    t_search = 0.0\n",
    "    t_rerank = 0.0\n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Evaluating MRR@5\", unit=\"query\"):\n",
    "        q = row[\"query\"]\n",
    "        gold_id = row[\"id\"]\n",
    "\n",
    "        t0 = time.perf_counter()\n",
    "        _, I = rag.search(q, k=retrieve_k)   # FAISS topK\n",
    "        t1 = time.perf_counter()\n",
    "\n",
    "        cand_idxs = [int(x) for x in I[0]]\n",
    "        cand_docs = [rag.doc_store[i] for i in cand_idxs]\n",
    "        cand_texts = [d.page_content for d in cand_docs]\n",
    "\n",
    "        scores = rag.rerank(q, cand_texts, batch_size=rr_batch_size)\n",
    "        t2 = time.perf_counter()\n",
    "\n",
    "        # сортируем по score desc и берём топ-5\n",
    "        ranked = sorted(zip(cand_docs, scores), key=lambda x: x[1], reverse=True)[:5]\n",
    "        ranked_ids = [d.metadata.get(\"id\") for d, _ in ranked]\n",
    "\n",
    "        # reciprocal rank\n",
    "        rr = 0.0\n",
    "        for rank, rid in enumerate(ranked_ids, start=1):\n",
    "            if rid == gold_id:\n",
    "                rr = 1.0 / rank\n",
    "                break\n",
    "        mrrs.append(rr)\n",
    "\n",
    "        t_search += (t1 - t0)\n",
    "        t_rerank += (t2 - t1)\n",
    "\n",
    "    return {\n",
    "        \"n\": len(df),\n",
    "        \"MRR@5\": float(np.mean(mrrs)),\n",
    "        \"avg_faiss_search_s\": t_search / len(df),\n",
    "        \"avg_rerank_s\": t_rerank / len(df),\n",
    "        \"avg_total_s\": (t_search + t_rerank) / len(df),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7390e040",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating MRR@5:   0%|          | 0/1000 [00:00<?, ?query/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "Evaluating MRR@5: 100%|██████████| 1000/1000 [29:48<00:00,  1.79s/query]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n': 1000, 'MRR@5': 0.9758166666666666, 'avg_faiss_search_s': 0.05542807636800535, 'avg_rerank_s': 1.732118324302026, 'avg_total_s': 1.7875464006700312}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "res = mrr_at_5(rag, \"test_sample.csv\", retrieve_k=50, rr_batch_size=4)\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe7827a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TOP результаты:\n",
      "\n",
      "#1  score=0.9997  id=0704.0771\n",
      "Suppression of 1/f noise in one-qubit systems\n",
      "We investigate the generation of quantum operations for one-qubit systems\n",
      "under classical noise with 1/f^\\alpha power spectrum, where 2>\\alpha > 0. We\n",
      "present an efficient way to approximate the noise with a discrete multi-state\n",
      "Markovian fluctuator. With this method, the average temporal evolution of the\n",
      "qubit density matrix under 1/f^\\alpha noise can be feasibly determined from\n",
      "recently derived deterministic master equations. We obtain qubit operations\n",
      "such as quantum memory and the NOT}gate to high fidelity by a gradient based\n",
      "optimization algorithm. For the NOT gate, the computed fidelities are\n",
      "qualitatively similar to those obtained earlier for random telegraph noise. In\n",
      "the case of quantum memory however, we observe a nonmonotonic dependency of the\n",
      "fidelity on the operation time, yielding a natural access rate of the memory.\n",
      "\n",
      "#2  score=0.9992  id=quant-ph/0412169\n",
      "Generation of quantum logic operations from physical Hamiltonians\n",
      "We provide a systematic analysis of the physical generation of single- and\n",
      "two-qubit quantum operations from Hamiltonians available in various quantum\n",
      "systems for scalable quantum information processing. We show that generation of\n",
      "one-qubit operations can be transformed into a steering problem on the Bloch\n",
      "sphere, whereas the two-qubit problem can be generally transformed into a\n",
      "steering problem in a tetrahedron representing all the local equivalence\n",
      "classes of two-qubit operations (the Weyl chamber). We use this approach to\n",
      "investigate several physical examples for the generation of two-qubit\n",
      "operations. The steering approach provides useful guidance for the realization\n",
      "of various quantum computation schemes.\n",
      "\n",
      "#3  score=0.9976  id=1506.04992\n",
      "A controllable single photon beam-splitter as a node of a quantum\n",
      "  network\n",
      "A model for a controlled single-photon beam-splitter is proposed and\n",
      "analysed. It consists of two crossed optical-cavities with overlapping waists,\n",
      "dynamically coupled to a single flying atom. The system is shown to route a\n",
      "single photon with near-unity efficiency in an effective \"weak-coupling\"\n",
      "regime. Furthermore, two such nodes, forming a segment of a quantum network,\n",
      "are shown to perform several controlled quantum operations. All one-qubit\n",
      "operations involve a transfer of a photon from one cavity to another in a\n",
      "single node, while two-qubit operations involve transfer from one node to a\n",
      "next one, coupled via an optical fiber. Novel timing protocols for classical\n",
      "optical fields are found to simplify possible experimental realizations along\n",
      "with achievable effective parameter regime. Though our analysis here is\n",
      "restricted to a cavity-QED scenario, basic features of the model can be\n",
      "extended to various other physical systems including gated quantum dots,\n",
      "circuit-QED or opto-mechanical elements.\n",
      "\n",
      "#4  score=0.9929  id=2009.03110\n",
      "Thermal Operations in general are not memoryless\n",
      "So-called Thermal Operations seem to describe the most fundamental, and\n",
      "reasonable, set of operations allowable for state transformations at an ambient\n",
      "inverse temperature $\\beta$. However, a priori, they require experimentalists\n",
      "to manipulate very complex environments and have control over their internal\n",
      "degrees of freedom. For this reason, the community has been working on creating\n",
      "more experimentally-friendly operations. In [Perry et al., Phys. Rev. X 8,\n",
      "041049] it was shown that for states diagonal in the energy basis, that Thermal\n",
      "Operations can be performed by so-called Coarse Operations, which need just one\n",
      "auxiliary qubit, but are otherwise Markovian and classical in spirit. In this\n",
      "work, by providing an explicit counterexample, we show that this one qubit of\n",
      "memory is necessary. We also fully characterize the possible transitions that\n",
      "do not require memory for the system being a qubit. We do this by analyzing\n",
      "arbitrary control sequences comprising level energy changes and partial\n",
      "thermalizations in each step.\n",
      "\n",
      "#5  score=0.9901  id=1805.05305\n",
      "Transforming graph states using single-qubit operations\n",
      "Stabilizer states form an important class of states in quantum information,\n",
      "and are of central importance in quantum error correction. Here, we provide an\n",
      "algorithm for deciding whether one stabilizer (target) state can be obtained\n",
      "from another stabilizer (source) state by single-qubit Clifford operations\n",
      "(LC), single-qubit Pauli measurements (LPM), and classical communication (CC)\n",
      "between sites holding the individual qubits. What's more, we provide a recipe\n",
      "to obtain the sequence of LC+LPM+CC operations which prepare the desired target\n",
      "state from the source state, and show how these operations can be applied in\n",
      "parallel to reach the target state in constant time. Our algorithm has\n",
      "applications in quantum networks, quantum computing, and can also serve as a\n",
      "design tool - for example, to find transformations between quantum error\n",
      "correcting codes. We provide a software implementation of our algorithm that\n",
      "makes this tool easier to apply.\n",
      "  A key insight leading to our algorithm is to show that the problem is\n",
      "equivalent to one in graph theory, which is to decide whether some graph G' is\n",
      "a vertex-minor of another graph G. Here we\n"
     ]
    }
   ],
   "source": [
    "q = 'quantum operations for one-qubit'\n",
    "\n",
    "retrieve_k = 50   # сколько брать из FAISS до rerank\n",
    "final_k = 5\n",
    "\n",
    "# 1) retrieve\n",
    "_, I = rag.search(q, k=retrieve_k)\n",
    "idxs = [int(x) for x in I[0]]\n",
    "docs = [rag.doc_store[i] for i in idxs]\n",
    "texts = [d.page_content for d in docs]\n",
    "\n",
    "# 2) rerank\n",
    "scores = rag.rerank(q, texts, batch_size=4)\n",
    "\n",
    "# 3) top-k после rerank\n",
    "ranked = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)[:final_k]\n",
    "\n",
    "print(\"\\nTOP результаты:\")\n",
    "for rank, (doc, sc) in enumerate(ranked, start=1):\n",
    "    print(f\"\\n#{rank}  score={sc:.4f}  id={doc.metadata.get('id')}\")\n",
    "    print(doc.page_content[:1200])  # чтобы не печатать слишком много\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
