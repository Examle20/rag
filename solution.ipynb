{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a5ff1d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Optional\n",
    "import os\n",
    "import faiss\n",
    "import numpy as np\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "import ijson\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa911cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"arxiv-metadata-s.json\"\n",
    "MODEL = \"Qwen/Qwen3-Embedding-0.6B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL, use_fast=True)\n",
    "\n",
    "lens = []\n",
    "with open(PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    for obj in ijson.items(f, \"item\"):\n",
    "        title = (obj.get(\"title\") or \"\").strip()\n",
    "        abstract = (obj.get(\"abstract\") or \"\").strip()\n",
    "        text = (title + \"\\n\" + abstract).strip()\n",
    "        ids = tokenizer(text, add_special_tokens=True, truncation=False)[\"input_ids\"]\n",
    "        lens.append(len(ids))\n",
    "\n",
    "arr = np.array(lens, dtype=np.int32)\n",
    "print(\"count:\", arr.size)\n",
    "print(\"mean tokens:\", float(arr.mean()))\n",
    "for p in [50, 90, 95, 99, 99.5, 99.9]:\n",
    "    print(f\"p{p}:\", float(np.percentile(arr, p)))\n",
    "print(\"max:\", int(arr.max()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2663b6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "class RAG:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedder_name: str = \"Qwen/Qwen3-Embedding-0.6B\",\n",
    "        reranker_name: str = \"Qwen/Qwen3-Reranker-0.6B\",\n",
    "        chunk_size: int = 500,\n",
    "        chunk_overlap: int = 125,\n",
    "        device: Optional[str] = None,\n",
    "    ):\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.emb_tokenizer = AutoTokenizer.from_pretrained(embedder_name)\n",
    "        self.embedder = AutoModel.from_pretrained(embedder_name).to(self.device)\n",
    "        self.embedder.eval()\n",
    "\n",
    "        self.rr_tokenizer = AutoTokenizer.from_pretrained(reranker_name, padding_side='left')\n",
    "        self.reranker = AutoModelForCausalLM.from_pretrained(reranker_name).to(self.device)\n",
    "        self.reranker.eval()\n",
    "\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap,)\n",
    "        self.index = None\n",
    "        self.doc_store = []\n",
    "\n",
    "        self.max_length = 1024\n",
    "        self.token_false_id = self.rr_tokenizer.convert_tokens_to_ids(\"no\")\n",
    "        self.token_true_id = self.rr_tokenizer.convert_tokens_to_ids(\"yes\")\n",
    "        prefix = \"<|im_start|>system\\nJudge whether the Document meets the requirements based on the Query and the Instruct provided. Note that the answer can only be \\\"yes\\\" or \\\"no\\\".<|im_end|>\\n<|im_start|>user\\n\"\n",
    "        suffix = \"<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n\"\n",
    "        self.prefix_tokens = self.rr_tokenizer.encode(prefix, add_special_tokens=False)\n",
    "        self.suffix_tokens = self.rr_tokenizer.encode(suffix, add_special_tokens=False)\n",
    "\n",
    "    def _generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        inputs = self.emb_tokenizer(\n",
    "            texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=self.max_length,\n",
    "        ).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.embedder(**inputs)\n",
    "\n",
    "        embeddings = self.last_token_pool(outputs.last_hidden_state, inputs.attention_mask)\n",
    "        embeddings = embeddings.float().cpu()  # <-- ключевая строка\n",
    "\n",
    "        return F.normalize(embeddings, p=2, dim=1).numpy()\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def last_token_pool(last_hidden_states: Tensor,\n",
    "                        attention_mask: Tensor) -> Tensor:\n",
    "        left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n",
    "        if left_padding:\n",
    "            return last_hidden_states[:, -1]\n",
    "        else:\n",
    "            sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "            batch_size = last_hidden_states.shape[0]\n",
    "            return last_hidden_states[\n",
    "                torch.arange(batch_size, device=last_hidden_states.device),\n",
    "                sequence_lengths] \n",
    "\n",
    "    def load_and_process_arxiv_json(self, file_path: str, split: bool = False) -> List[Document]:\n",
    "        ext = os.path.splitext(file_path)[1].lower()\n",
    "        if ext != \".json\":\n",
    "            raise ValueError(f\"Expected .json file, got: {ext}\")\n",
    "\n",
    "        docs: List[Document] = []\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for obj in ijson.items(f, \"item\"):\n",
    "                arxiv_id = obj.get(\"id\")\n",
    "                title = (obj.get(\"title\") or \"\").strip()\n",
    "                abstract = (obj.get(\"abstract\") or \"\").strip()\n",
    "                text = (title + \"\\n\" + abstract).strip()\n",
    "                meta = {\n",
    "                    \"id\": arxiv_id,\n",
    "                    \"title\": title,\n",
    "                    \"categories\": obj.get(\"categories\"),\n",
    "                    \"doi\": obj.get(\"doi\"),\n",
    "                    \"journal_ref\": obj.get(\"journal-ref\"),\n",
    "                    \"update_date\": obj.get(\"update_date\"),\n",
    "                }\n",
    "\n",
    "                docs.append(Document(page_content=text, metadata=meta))\n",
    "\n",
    "        return self.text_splitter.split_documents(docs) if split else docs\n",
    "\n",
    "    def build_index(self, file_path: str, batch_size: int = 64) -> None:\n",
    "        all_docs = self.load_and_process_arxiv_json(file_path, split=False)\n",
    "        self.doc_store = all_docs\n",
    "        embs = []\n",
    "        for i in tqdm(range(0, len(all_docs), batch_size), desc=\"Embedding corpus\", unit=\"batch\"):\n",
    "            batch_texts = [d.page_content for d in all_docs[i:i + batch_size]]\n",
    "            embs.append(self._generate_embeddings(batch_texts))\n",
    "        embeddings = np.concatenate(embs, axis=0).astype(\"float32\")\n",
    "        self.index = faiss.IndexFlatIP(embeddings.shape[1])\n",
    "        self.index.add(embeddings)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_detailed_instruct(task_description: str, query: str):\n",
    "        return f'Instruct: {task_description}\\nQuery:{query}'\n",
    "\n",
    "    @staticmethod\n",
    "    def format_reranker_instruction(query, doc, instruction=None):\n",
    "        if instruction is None:\n",
    "            instruction = 'Given a web search query, retrieve relevant passages that answer the query'\n",
    "        output = \"<Instruct>: {instruction}\\n<Query>: {query}\\n<Document>: {doc}\".format(\n",
    "            instruction=instruction, query=query, doc=doc)\n",
    "        return output\n",
    "\n",
    "    def process_inputs(self, pairs):\n",
    "        \"\"\"Обработка данных для реранкера\"\"\"\n",
    "        inputs = self.rr_tokenizer(pairs,\n",
    "                                   padding=False,\n",
    "                                   truncation='longest_first',\n",
    "                                   return_attention_mask=False,\n",
    "                                   max_length=self.max_length -\n",
    "                                   len(self.prefix_tokens) -\n",
    "                                   len(self.suffix_tokens))\n",
    "        for i, ele in enumerate(inputs['input_ids']):\n",
    "            inputs['input_ids'][\n",
    "                i] = self.prefix_tokens + ele + self.suffix_tokens\n",
    "        inputs = self.rr_tokenizer.pad(inputs,\n",
    "                                       padding=True,\n",
    "                                       return_tensors=\"pt\",\n",
    "                                       max_length=self.max_length)\n",
    "\n",
    "        # переносим тензоры на девайс ранжирующей модели\n",
    "        for key in inputs:\n",
    "            inputs[key] = inputs[key].to(self.device)\n",
    "        return inputs\n",
    "\n",
    "    def search(self,\n",
    "               query: str,\n",
    "               k: int = 5,\n",
    "               task: str = None):\n",
    "        if self.index is None:\n",
    "            raise ValueError(\"Index not initialized\")\n",
    "\n",
    "        if task is None:\n",
    "            task = 'Given a web search query, retrieve relevant passages that answer the query'\n",
    "\n",
    "        query_embedding = self._generate_embeddings([query])\n",
    "        distances, indices = self.index.search(query_embedding, k)\n",
    "        return distances, indices         \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def compute_logits(self, inputs):\n",
    "        batch_scores = self.reranker(**inputs).logits[:, -1, :]\n",
    "        true_vector = batch_scores[:, self.token_true_id]\n",
    "        false_vector = batch_scores[:, self.token_false_id]\n",
    "        batch_scores = torch.stack([false_vector, true_vector], dim=1)\n",
    "        batch_scores = torch.nn.functional.log_softmax(batch_scores, dim=1)\n",
    "        scores = batch_scores[:, 1].exp().tolist()\n",
    "        return scores\n",
    "\n",
    "    def rerank(self, query: str, documents: List[str], batch_size=4):\n",
    "        pairs = []\n",
    "        for d in documents:\n",
    "            pairs.append(self.format_reranker_instruction(query, d))\n",
    "\n",
    "        scores = []\n",
    "        for i in range(0, len(pairs), batch_size):\n",
    "            inputs = self.process_inputs(pairs[i:i + batch_size])\n",
    "            sc = self.compute_logits(inputs)\n",
    "            scores.extend(sc)\n",
    "        return scores\n",
    "    \n",
    "def save(self, prefix: str) -> None:\n",
    "    if self.index is None:\n",
    "        raise ValueError(\"Index not initialized\")\n",
    "    faiss.write_index(self.index, prefix + \".index\")\n",
    "    with open(prefix + \".docstore.pkl\", \"wb\") as f:\n",
    "        pickle.dump(self.doc_store, f)\n",
    "\n",
    "def load(self, prefix: str) -> None:\n",
    "    self.index = faiss.read_index(prefix + \".index\")\n",
    "    with open(prefix + \".docstore.pkl\", \"rb\") as f:\n",
    "        self.doc_store = pickle.load(f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6619bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding corpus: 100%|██████████| 1535/1535 [49:12<00:00,  1.92s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Towards a Microscopic Theory for Metallic Heavy-Fermion Point Contacts\n",
      "The bias-dependent resistance R(V) of NS-junctions is calculated using the\n",
      "Keldysh formalism in all orders of the transfer matrix element. We present a\n",
      "compact and simple formula for the Andreev current, that results from the\n",
      "coupling of electrons and holes on the normal side via the anomalous Green's\n",
      "function on the superconducting side. Using simple BCS Nambu-Green's functions\n",
      "the well known Blonder-Tinkam-Klapwijk theory can be recovered. Incorporating\n",
      "the energy-dependent quasi-particle lifetime of the heavy fermions strongly\n",
      "reduces the Andreev-reflection signal.\n",
      "-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#\n",
      "\n",
      "Chiral Dynamics and Heavy-Fermion Formalism in Nuclei: I. Exchange Axial\n",
      "  Currents\n",
      "Chiral perturbation theory in heavy-fermion formalism is developed for\n",
      "meson-exchange currents in nuclei and applied to nuclear axial- charge\n",
      "transitions. Calculation is performed to the next-to-leading order in chiral\n",
      "expansion which involves graphs up to one loop. The result turns out to be very\n",
      "simple. The previously conjectured notion of \"chiral filter mechanism\" in the\n",
      "time component of the nuclear axial current and the space component of the\n",
      "nuclear electromagnetic current is verified to that order. As a consequence,\n",
      "the phenomenologically observed soft-pion dominance in the nuclear process is\n",
      "given a simple interpretation in terms of chiral symmetry in nuclei. In this\n",
      "paper, we focus on the axial cur\n",
      "-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#\n",
      "\n",
      "Generalized fermion symmetry, its currents algebra and Ward-Takahashi\n",
      "  identities\n",
      "We introduce a new local symmetry into the fermion sector of a gauge\n",
      "invariant Lagrangian which may or may not contain a scalar or spontaneous\n",
      "symmetry breaking. The Standard Model in the unitary gauge and QCD are\n",
      "particular cases where this symmetry may apply. We determine the associated\n",
      "vector and axial vector currents and their conservation laws. We show that a\n",
      "single current conservation law may lead to multiple Ward-Takahashi identities.\n",
      "Our results can potentially have important consequences for effective models of\n",
      "low-energy QCD and hadron structure. As an specific example, we discuss the\n",
      "construction of tetraquark states within a generalized linear sigma model and\n",
      "show that this new symmetry probes t\n",
      "-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#\n",
      "\n",
      "Hadamard States and Adiabatic Vacua\n",
      "Reversing a slight detrimental effect of the mailer related to TeXability\n",
      "-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#\n",
      "\n",
      "On the Early History of Current Algebra\n",
      "The history of Current Algebra is reviewed up to the appearance of the\n",
      "Adler-Weisberger sum rule. Particular emphasis is given to the role current\n",
      "algebra played for the historical struggle in strong interaction physics of\n",
      "elementary particles between the S-matrix approach based on dispersion\n",
      "relations and field theory. The question whether there are fundamental\n",
      "particles or all hadrons are bound or resonant states of one another played an\n",
      "important role in this struggle and is thus also regarded.\n",
      "-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q = \"Keldysh formalism Andreev current heavy fermions\"\n",
    "\n",
    "k = 5\n",
    "rag = RAG(device=\"cuda\")\n",
    "rag.build_index(\"./arxiv-metadata-s.json\")\n",
    "\n",
    "D, I = rag.search(q, k=k)\n",
    "candidates = [rag.doc_store[i].page_content for i in I[0]]\n",
    "\n",
    "for c in candidates:\n",
    "    print(c[:800])\n",
    "    print(\"-#\" * 20)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4804781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faiss dim: 1024\n",
      "index vectors: 98213\n",
      "approx faiss vectors size (MB): 383.64453125\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "ntotal = rag.index.ntotal\n",
    "dim = rag.index.d\n",
    "bytes_index = ntotal * dim * 4  # float32 = 4 bytes\n",
    "\n",
    "print(\"faiss dim:\", dim)\n",
    "print(\"index vectors:\", ntotal)\n",
    "print(\"approx faiss vectors size (MB):\", bytes_index / (1024**2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "631b3699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: arxiv_rag_qwen3.index and arxiv_rag_qwen3.docstore.pkl\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import pickle\n",
    "\n",
    "prefix = \"arxiv_rag_qwen3\"\n",
    "\n",
    "# 1) сохранить FAISS индекс\n",
    "faiss.write_index(rag.index, prefix + \".index\")\n",
    "\n",
    "# 2) сохранить doc_store (список Document)\n",
    "with open(prefix + \".docstore.pkl\", \"wb\") as f:\n",
    "    pickle.dump(rag.doc_store, f)\n",
    "\n",
    "print(\"Saved:\", prefix + \".index\", \"and\", prefix + \".docstore.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "371645ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def profile_query(rag, query: str, retrieve_k: int = 50, rr_batch_size: int = 4):\n",
    "    if rag.index is None:\n",
    "        raise ValueError(\"Index not initialized\")\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    q_emb = rag._generate_embeddings([query])\n",
    "    t1 = time.perf_counter()\n",
    "\n",
    "    D, I = rag.index.search(q_emb.astype(\"float32\"), retrieve_k)\n",
    "    t2 = time.perf_counter()\n",
    "\n",
    "    idxs = [int(x) for x in I[0]]\n",
    "    cand_docs = [rag.doc_store[i] for i in idxs]\n",
    "    cand_texts = [d.page_content for d in cand_docs]\n",
    "    t3 = time.perf_counter()\n",
    "\n",
    "    scores = rag.rerank(query, cand_texts, batch_size=rr_batch_size)\n",
    "    t4 = time.perf_counter()\n",
    "\n",
    "    ranked = sorted(zip(idxs, scores), key=lambda x: x[1], reverse=True)[:5]\n",
    "    t5 = time.perf_counter()\n",
    "\n",
    "    return {\n",
    "        \"query_embed_s\": t1 - t0,\n",
    "        \"faiss_search_s\": t2 - t1,\n",
    "        \"gather_candidates_s\": t3 - t2,\n",
    "        \"rerank_s\": t4 - t3,\n",
    "        \"sort_top5_s\": t5 - t4,\n",
    "        \"total_s\": t5 - t0,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa1d2ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query_embed_s': 0.03334781900048256, 'faiss_search_s': 0.0300979189996724, 'gather_candidates_s': 9.762999980011955e-05, 'rerank_s': 1.567501509000067, 'sort_top5_s': 2.010000025620684e-05, 'total_s': 1.6310649770002783}\n"
     ]
    }
   ],
   "source": [
    "profile = profile_query(rag, \"attention mechanism in transformers\", retrieve_k=50, rr_batch_size=4)\n",
    "print(profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fcfe670",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def mrr_at_5(rag, test_csv_path: str, retrieve_k: int = 50, rr_batch_size: int = 4, limit: int | None = None):\n",
    "    df = pd.read_csv(test_csv_path)\n",
    "    if limit is not None:\n",
    "        df = df.head(limit)\n",
    "\n",
    "    mrrs = []\n",
    "    t_search = 0.0\n",
    "    t_rerank = 0.0\n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Evaluating MRR@5\", unit=\"query\"):\n",
    "        q = row[\"query\"]\n",
    "        gold_id = row[\"id\"]\n",
    "\n",
    "        t0 = time.perf_counter()\n",
    "        _, I = rag.search(q, k=retrieve_k)   # FAISS topK\n",
    "        t1 = time.perf_counter()\n",
    "\n",
    "        cand_idxs = [int(x) for x in I[0]]\n",
    "        cand_docs = [rag.doc_store[i] for i in cand_idxs]\n",
    "        cand_texts = [d.page_content for d in cand_docs]\n",
    "\n",
    "        scores = rag.rerank(q, cand_texts, batch_size=rr_batch_size)\n",
    "        t2 = time.perf_counter()\n",
    "\n",
    "        # сортируем по score desc и берём топ-5\n",
    "        ranked = sorted(zip(cand_docs, scores), key=lambda x: x[1], reverse=True)[:5]\n",
    "        ranked_ids = [d.metadata.get(\"id\") for d, _ in ranked]\n",
    "\n",
    "        # reciprocal rank\n",
    "        rr = 0.0\n",
    "        for rank, rid in enumerate(ranked_ids, start=1):\n",
    "            if rid == gold_id:\n",
    "                rr = 1.0 / rank\n",
    "                break\n",
    "        mrrs.append(rr)\n",
    "\n",
    "        t_search += (t1 - t0)\n",
    "        t_rerank += (t2 - t1)\n",
    "\n",
    "    return {\n",
    "        \"n\": len(df),\n",
    "        \"MRR@5\": float(np.mean(mrrs)),\n",
    "        \"avg_faiss_search_s\": t_search / len(df),\n",
    "        \"avg_rerank_s\": t_rerank / len(df),\n",
    "        \"avg_total_s\": (t_search + t_rerank) / len(df),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7390e040",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating MRR@5:   0%|          | 0/50 [00:00<?, ?query/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "Evaluating MRR@5: 100%|██████████| 50/50 [01:31<00:00,  1.83s/query]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n': 50, 'MRR@5': 0.9866666666666666, 'avg_faiss_search_s': 0.06016224915994826, 'avg_rerank_s': 1.767476377959938, 'avg_total_s': 1.8276386271198861}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "res = mrr_at_5(rag, \"test_sample.csv\", retrieve_k=50, rr_batch_size=4, limit=50)\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7827a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = input(\"Введите запрос: \").strip()\n",
    "\n",
    "retrieve_k = 50   # сколько брать из FAISS до rerank\n",
    "final_k = 5\n",
    "\n",
    "# 1) retrieve\n",
    "_, I = rag.search(q, k=retrieve_k)\n",
    "idxs = [int(x) for x in I[0]]\n",
    "docs = [rag.doc_store[i] for i in idxs]\n",
    "texts = [d.page_content for d in docs]\n",
    "\n",
    "# 2) rerank\n",
    "scores = rag.rerank(q, texts, batch_size=4)\n",
    "\n",
    "# 3) top-k после rerank\n",
    "ranked = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)[:final_k]\n",
    "\n",
    "print(\"\\nTOP результаты:\")\n",
    "for rank, (doc, sc) in enumerate(ranked, start=1):\n",
    "    print(f\"\\n#{rank}  score={sc:.4f}  id={doc.metadata.get('id')}\")\n",
    "    print(doc.page_content[:1200])  # чтобы не печатать слишком много\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
