{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5ff1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "import faiss\n",
    "import numpy as np\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import ijson\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa911cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"arxiv-metadata-s.json\"\n",
    "MODEL = \"Qwen/Qwen3-Embedding-0.6B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL, use_fast=True)\n",
    "\n",
    "lens = []\n",
    "with open(PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    for obj in ijson.items(f, \"item\"):\n",
    "        title = (obj.get(\"title\") or \"\").strip()\n",
    "        abstract = (obj.get(\"abstract\") or \"\").strip()\n",
    "        text = (title + \"\\n\" + abstract).strip()\n",
    "        ids = tokenizer(text, add_special_tokens=True, truncation=False)[\"input_ids\"]\n",
    "        lens.append(len(ids))\n",
    "\n",
    "arr = np.array(lens, dtype=np.int32)\n",
    "print(\"count:\", arr.size)\n",
    "print(\"mean tokens:\", float(arr.mean()))\n",
    "for p in [50, 90, 95, 99, 99.5, 99.9]:\n",
    "    print(f\"p{p}:\", float(np.percentile(arr, p)))\n",
    "print(\"max:\", int(arr.max()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2663b6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAG:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedder_name: str = \"Qwen/Qwen3-Embedding-0.6B\",\n",
    "        reranker_name: str = \"Qwen/Qwen3-Reranker-0.6B\",\n",
    "        chunk_size: int = 500,\n",
    "        chunk_overlap: int = 125,\n",
    "        device: Optional[str] = None,\n",
    "    ):\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.emb_tokenizer = AutoTokenizer.from_pretrained(embedder_name)\n",
    "        self.embedder = AutoModel.from_pretrained(embedder_name).to(self.device)\n",
    "        self.embedder.eval()\n",
    "\n",
    "        self.rr_tokenizer = AutoTokenizer.from_pretrained(reranker_name, padding_side='left')\n",
    "        self.reranker = AutoModelForCausalLM.from_pretrained(reranker_name).to(self.device)\n",
    "        self.reranker.eval()\n",
    "\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap,)\n",
    "        self.index = None\n",
    "        self.doc_store = []\n",
    "\n",
    "        self.max_length = 1024\n",
    "        self.token_false_id = self.rr_tokenizer.convert_tokens_to_ids(\"no\")\n",
    "        self.token_true_id = self.rr_tokenizer.convert_tokens_to_ids(\"yes\")\n",
    "        prefix = \"<|im_start|>system\\nJudge whether the Document meets the requirements based on the Query and the Instruct provided. Note that the answer can only be \\\"yes\\\" or \\\"no\\\".<|im_end|>\\n<|im_start|>user\\n\"\n",
    "        suffix = \"<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n\"\n",
    "        self.prefix_tokens = self.rr_tokenizer.encode(prefix, add_special_tokens=False)\n",
    "        self.suffix_tokens = self.rr_tokenizer.encode(suffix, add_special_tokens=False)\n",
    "\n",
    "    def _generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        inputs = self.emb_tokenizer(\n",
    "            texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=self.max_length,\n",
    "        ).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.embedder(**inputs)\n",
    "\n",
    "        embeddings = self.last_token_pool(outputs.last_hidden_state, inputs.attention_mask)\n",
    "        embeddings = embeddings.float().cpu()  # <-- ключевая строка\n",
    "\n",
    "        return F.normalize(embeddings, p=2, dim=1).numpy()\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def last_token_pool(last_hidden_states: Tensor,\n",
    "                        attention_mask: Tensor) -> Tensor:\n",
    "        left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n",
    "        if left_padding:\n",
    "            return last_hidden_states[:, -1]\n",
    "        else:\n",
    "            sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "            batch_size = last_hidden_states.shape[0]\n",
    "            return last_hidden_states[\n",
    "                torch.arange(batch_size, device=last_hidden_states.device),\n",
    "                sequence_lengths] \n",
    "\n",
    "    def load_and_process_arxiv_json(self, file_path: str, split: bool = False) -> List[Document]:\n",
    "        ext = os.path.splitext(file_path)[1].lower()\n",
    "        if ext != \".json\":\n",
    "            raise ValueError(f\"Expected .json file, got: {ext}\")\n",
    "\n",
    "        docs: List[Document] = []\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for obj in ijson.items(f, \"item\"):\n",
    "                arxiv_id = obj.get(\"id\")\n",
    "                title = (obj.get(\"title\") or \"\").strip()\n",
    "                abstract = (obj.get(\"abstract\") or \"\").strip()\n",
    "                text = (title + \"\\n\" + abstract).strip()\n",
    "                meta = {\n",
    "                    \"id\": arxiv_id,\n",
    "                    \"title\": title,\n",
    "                    \"categories\": obj.get(\"categories\"),\n",
    "                    \"doi\": obj.get(\"doi\"),\n",
    "                    \"journal_ref\": obj.get(\"journal-ref\"),\n",
    "                    \"update_date\": obj.get(\"update_date\"),\n",
    "                }\n",
    "\n",
    "                docs.append(Document(page_content=text, metadata=meta))\n",
    "\n",
    "        return self.text_splitter.split_documents(docs) if split else docs\n",
    "\n",
    "    def build_index(self, file_path: str, batch_size: int = 64) -> None:\n",
    "        all_docs = self.load_and_process_arxiv_json(file_path, split=False)\n",
    "        self.doc_store = all_docs\n",
    "        embs = []\n",
    "        for i in tqdm(range(0, len(all_docs), batch_size), desc=\"Embedding corpus\", unit=\"batch\"):\n",
    "            batch_texts = [d.page_content for d in all_docs[i:i + batch_size]]\n",
    "            embs.append(self._generate_embeddings(batch_texts))\n",
    "        embeddings = np.concatenate(embs, axis=0).astype(\"float32\")\n",
    "        self.index = faiss.IndexFlatIP(embeddings.shape[1])\n",
    "        self.index.add(embeddings)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_detailed_instruct(task_description: str, query: str):\n",
    "        return f'Instruct: {task_description}\\nQuery:{query}'\n",
    "\n",
    "    @staticmethod\n",
    "    def format_reranker_instruction(query, doc, instruction=None):\n",
    "        if instruction is None:\n",
    "            instruction = 'Given a web search query, retrieve relevant passages that answer the query'\n",
    "        output = \"<Instruct>: {instruction}\\n<Query>: {query}\\n<Document>: {doc}\".format(\n",
    "            instruction=instruction, query=query, doc=doc)\n",
    "        return output\n",
    "\n",
    "    def process_inputs(self, pairs):\n",
    "        \"\"\"Обработка данных для реранкера\"\"\"\n",
    "        inputs = self.rr_tokenizer(pairs,\n",
    "                                   padding=False,\n",
    "                                   truncation='longest_first',\n",
    "                                   return_attention_mask=False,\n",
    "                                   max_length=self.max_length -\n",
    "                                   len(self.prefix_tokens) -\n",
    "                                   len(self.suffix_tokens))\n",
    "        for i, ele in enumerate(inputs['input_ids']):\n",
    "            inputs['input_ids'][\n",
    "                i] = self.prefix_tokens + ele + self.suffix_tokens\n",
    "        inputs = self.rr_tokenizer.pad(inputs,\n",
    "                                       padding=True,\n",
    "                                       return_tensors=\"pt\",\n",
    "                                       max_length=self.max_length)\n",
    "\n",
    "        # переносим тензоры на девайс ранжирующей модели\n",
    "        for key in inputs:\n",
    "            inputs[key] = inputs[key].to(self.device)\n",
    "        return inputs\n",
    "\n",
    "    def search(self,\n",
    "               query: str,\n",
    "               k: int = 5,\n",
    "               task: str = None):\n",
    "        if self.index is None:\n",
    "            raise ValueError(\"Index not initialized\")\n",
    "\n",
    "        if task is None:\n",
    "            task = 'Given a web search query, retrieve relevant passages that answer the query'\n",
    "\n",
    "        query_embedding = self._generate_embeddings([query])\n",
    "        distances, indices = self.index.search(query_embedding, k)\n",
    "        return distances, indices         \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def compute_logits(self, inputs):\n",
    "        batch_scores = self.reranker(**inputs).logits[:, -1, :]\n",
    "        true_vector = batch_scores[:, self.token_true_id]\n",
    "        false_vector = batch_scores[:, self.token_false_id]\n",
    "        batch_scores = torch.stack([false_vector, true_vector], dim=1)\n",
    "        batch_scores = torch.nn.functional.log_softmax(batch_scores, dim=1)\n",
    "        scores = batch_scores[:, 1].exp().tolist()\n",
    "        return scores\n",
    "\n",
    "    def rerank(self, query: str, documents: List[str], batch_size=4):\n",
    "        pairs = []\n",
    "        for d in documents:\n",
    "            pairs.append(self.format_reranker_instruction(query, d))\n",
    "\n",
    "        scores = []\n",
    "        for i in range(0, len(pairs), batch_size):\n",
    "            inputs = self.process_inputs(pairs[i:i + batch_size])\n",
    "            sc = self.compute_logits(inputs)\n",
    "            scores.extend(sc)\n",
    "        return scores            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6619bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"Keldysh formalism Andreev current heavy fermions\"\n",
    "\n",
    "k = 5\n",
    "rag = RAG(device=\"cuda\")\n",
    "rag.build_index(\"./arxiv-metadata-s.json\")\n",
    "\n",
    "D, I = rag.search(q, k=k)\n",
    "candidates = [rag.doc_store[i].page_content for i in I[0]]\n",
    "\n",
    "for c in candidates:\n",
    "    print(c[:800])  # чтобы не печатать всё\n",
    "    print(\"-#\" * 20)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
